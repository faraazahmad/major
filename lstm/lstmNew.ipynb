{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "def create_lookup_table(vocab):\n",
    "    # make a list of unique characters\n",
    "    \n",
    "    vocab = set(list(vocab))\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "            vocab_to_int[v] = v_i\n",
    "            \n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_words, target_words, source_vocab_to_int, target_vocab_to_int):\n",
    "    \n",
    "        #1st, 2nd args: raw string text to be converted\n",
    "        #3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        #return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \n",
    "    # empty list of converted words\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    max_source_word_length = max([len(word) for word in source_words])\n",
    "    max_target_word_length = max([len(word) for word in target_words])\n",
    "    \n",
    "    # iterating through each word (# of words in source&target is the same)\n",
    "    for i in range(len(source_words)):\n",
    "        # extract words one by one\n",
    "        source_word = source_words[i]\n",
    "        target_word = target_words[i]\n",
    "        \n",
    "        # make a list of characters (extraction) from the chosen word\n",
    "        source_tokens = list(source_word)\n",
    "        target_tokens = list(target_word)\n",
    "        \n",
    "        # empty list of converted words to index in the chosen word\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target word\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted words in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(source_text, target_text):\n",
    "\n",
    "\n",
    "    Englishvocab = 'abcdefghijklmnopqrstuvwxyz'    \n",
    "    Hindivocab = 'ँंॉॆॊॏऺऻॎःािीुूेैोौअआइईउऊएऐओऔकखगघचछजझटठडढणतथदधनपफबभमयरलवशषसहज्ञक्षश्रज़रफ़ड़ढ़ख़क़ग़ळृृ़़ऑ'\n",
    "    # create lookup tables for English and Hindi data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_table(Hindivocab)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_table(Englishvocab)\n",
    "\n",
    "    # create list of words whose characters are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "    \n",
    "     # Save data for later use\n",
    "    #pickle.dump((\n",
    "        #(source_text, target_text),\n",
    "      #  (source_vocab_to_int, target_vocab_to_int),\n",
    "       # (source_int_to_vocab, target_int_to_vocab)), open('preprocessTest.p', 'wb'))\n",
    "    \n",
    "    return source_text,target_text,source_vocab_to_int,target_vocab_to_int,source_int_to_vocab,target_int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"transliteration.txt\",delimiter = \"\\t\",header=None,encoding='utf-8',na_filter = False)\n",
    "X = dataset.iloc[:,-1]\n",
    "y = dataset.iloc[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    with open('preprocessTest.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_int_text, target_int_text, source_vocab_to_int, target_vocab_to_int,source_int_to_vocab,target_int_to_vocab = preprocess(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asheeque/anaconda3/envs/lstm/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 200\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 30\n",
    "\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "encoding_embedding_size = 50\n",
    "decoding_embedding_size = 50\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d921e2350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d921e2350>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d921e2350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d921e2350>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d921c61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d921c61d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d921c61d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d921c61d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cbe28c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cbe28c2d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cbe28c2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cbe28c2d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f7d4cffa990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7d3bf77a50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7cd6da5d50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7d4c6c0ed0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "save_path = 'checkpointsNew2/dev'\n",
    "source_int_text, target_int_text, source_vocab_to_int, target_vocab_to_int,source_int_to_vocab,target_int_to_vocab = preprocess(X,y)\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_word_batch(word_batch, pad_int):\n",
    "    \n",
    "    max_word = max([len(word) for word in word_batch])\n",
    "    return [word + [pad_int] * (max_word - len(word)) for word in word_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_word_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_word_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
    "        \n",
    "def get_accuracy(target, logits):\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    \n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 200/1027 - Train Accuracy: 0.4267, Validation Accuracy: 0.3519, Loss: 2.0066\n",
      "Epoch   1 Batch 400/1027 - Train Accuracy: 0.3667, Validation Accuracy: 0.1889, Loss: 1.6517\n",
      "Epoch   1 Batch 600/1027 - Train Accuracy: 0.4667, Validation Accuracy: 0.3556, Loss: 1.4345\n",
      "Epoch   1 Batch 800/1027 - Train Accuracy: 0.4424, Validation Accuracy: 0.4148, Loss: 1.5155\n",
      "Epoch   1 Batch 1000/1027 - Train Accuracy: 0.5467, Validation Accuracy: 0.4815, Loss: 1.1068\n",
      "Epoch   2 Batch 200/1027 - Train Accuracy: 0.4867, Validation Accuracy: 0.5000, Loss: 1.3517\n",
      "Epoch   2 Batch 400/1027 - Train Accuracy: 0.5455, Validation Accuracy: 0.5407, Loss: 1.0412\n",
      "Epoch   2 Batch 600/1027 - Train Accuracy: 0.6697, Validation Accuracy: 0.6333, Loss: 0.9529\n",
      "Epoch   2 Batch 800/1027 - Train Accuracy: 0.5576, Validation Accuracy: 0.6407, Loss: 1.0882\n",
      "Epoch   2 Batch 1000/1027 - Train Accuracy: 0.6467, Validation Accuracy: 0.6630, Loss: 0.7956\n",
      "Epoch   3 Batch 200/1027 - Train Accuracy: 0.6167, Validation Accuracy: 0.6667, Loss: 1.0404\n",
      "Epoch   3 Batch 400/1027 - Train Accuracy: 0.7121, Validation Accuracy: 0.6963, Loss: 0.7768\n",
      "Epoch   3 Batch 600/1027 - Train Accuracy: 0.7697, Validation Accuracy: 0.7370, Loss: 0.7069\n",
      "Epoch   3 Batch 800/1027 - Train Accuracy: 0.6788, Validation Accuracy: 0.7444, Loss: 0.8570\n",
      "Epoch   3 Batch 1000/1027 - Train Accuracy: 0.7133, Validation Accuracy: 0.7407, Loss: 0.5183\n",
      "Epoch   4 Batch 200/1027 - Train Accuracy: 0.6433, Validation Accuracy: 0.7556, Loss: 0.8494\n",
      "Epoch   4 Batch 400/1027 - Train Accuracy: 0.7182, Validation Accuracy: 0.7593, Loss: 0.5456\n",
      "Epoch   4 Batch 600/1027 - Train Accuracy: 0.7636, Validation Accuracy: 0.7704, Loss: 0.6257\n",
      "Epoch   4 Batch 800/1027 - Train Accuracy: 0.7061, Validation Accuracy: 0.7667, Loss: 0.6733\n",
      "Epoch   4 Batch 1000/1027 - Train Accuracy: 0.7667, Validation Accuracy: 0.7407, Loss: 0.4696\n",
      "Epoch   5 Batch 200/1027 - Train Accuracy: 0.7100, Validation Accuracy: 0.7630, Loss: 0.7297\n",
      "Epoch   5 Batch 400/1027 - Train Accuracy: 0.8061, Validation Accuracy: 0.7556, Loss: 0.4479\n",
      "Epoch   5 Batch 600/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7556, Loss: 0.4574\n",
      "Epoch   5 Batch 800/1027 - Train Accuracy: 0.7242, Validation Accuracy: 0.7481, Loss: 0.5948\n",
      "Epoch   5 Batch 1000/1027 - Train Accuracy: 0.7667, Validation Accuracy: 0.7444, Loss: 0.4618\n",
      "Epoch   6 Batch 200/1027 - Train Accuracy: 0.7367, Validation Accuracy: 0.7630, Loss: 0.5935\n",
      "Epoch   6 Batch 400/1027 - Train Accuracy: 0.8121, Validation Accuracy: 0.7444, Loss: 0.3625\n",
      "Epoch   6 Batch 600/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7593, Loss: 0.4700\n",
      "Epoch   6 Batch 800/1027 - Train Accuracy: 0.8061, Validation Accuracy: 0.7444, Loss: 0.5281\n",
      "Epoch   6 Batch 1000/1027 - Train Accuracy: 0.7844, Validation Accuracy: 0.7481, Loss: 0.3946\n",
      "Epoch   7 Batch 200/1027 - Train Accuracy: 0.7167, Validation Accuracy: 0.7519, Loss: 0.5557\n",
      "Epoch   7 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7593, Loss: 0.2632\n",
      "Epoch   7 Batch 600/1027 - Train Accuracy: 0.8455, Validation Accuracy: 0.7667, Loss: 0.3866\n",
      "Epoch   7 Batch 800/1027 - Train Accuracy: 0.7909, Validation Accuracy: 0.7481, Loss: 0.3752\n",
      "Epoch   7 Batch 1000/1027 - Train Accuracy: 0.8289, Validation Accuracy: 0.7444, Loss: 0.3440\n",
      "Epoch   8 Batch 200/1027 - Train Accuracy: 0.7200, Validation Accuracy: 0.7333, Loss: 0.5255\n",
      "Epoch   8 Batch 400/1027 - Train Accuracy: 0.8273, Validation Accuracy: 0.7667, Loss: 0.2938\n",
      "Epoch   8 Batch 600/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7481, Loss: 0.3512\n",
      "Epoch   8 Batch 800/1027 - Train Accuracy: 0.7636, Validation Accuracy: 0.7556, Loss: 0.3529\n",
      "Epoch   8 Batch 1000/1027 - Train Accuracy: 0.8089, Validation Accuracy: 0.7481, Loss: 0.3459\n",
      "Epoch   9 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7519, Loss: 0.4863\n",
      "Epoch   9 Batch 400/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7778, Loss: 0.2603\n",
      "Epoch   9 Batch 600/1027 - Train Accuracy: 0.8273, Validation Accuracy: 0.7593, Loss: 0.3392\n",
      "Epoch   9 Batch 800/1027 - Train Accuracy: 0.7606, Validation Accuracy: 0.7481, Loss: 0.3711\n",
      "Epoch   9 Batch 1000/1027 - Train Accuracy: 0.8156, Validation Accuracy: 0.7481, Loss: 0.3233\n",
      "Epoch  10 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7407, Loss: 0.4944\n",
      "Epoch  10 Batch 400/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7667, Loss: 0.2361\n",
      "Epoch  10 Batch 600/1027 - Train Accuracy: 0.8242, Validation Accuracy: 0.7519, Loss: 0.3520\n",
      "Epoch  10 Batch 800/1027 - Train Accuracy: 0.7333, Validation Accuracy: 0.7444, Loss: 0.3209\n",
      "Epoch  10 Batch 1000/1027 - Train Accuracy: 0.8400, Validation Accuracy: 0.7556, Loss: 0.2783\n",
      "Epoch  11 Batch 200/1027 - Train Accuracy: 0.7700, Validation Accuracy: 0.7222, Loss: 0.4186\n",
      "Epoch  11 Batch 400/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7667, Loss: 0.2485\n",
      "Epoch  11 Batch 600/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7481, Loss: 0.2706\n",
      "Epoch  11 Batch 800/1027 - Train Accuracy: 0.7758, Validation Accuracy: 0.7481, Loss: 0.2777\n",
      "Epoch  11 Batch 1000/1027 - Train Accuracy: 0.8244, Validation Accuracy: 0.7481, Loss: 0.2970\n",
      "Epoch  12 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7630, Loss: 0.4372\n",
      "Epoch  12 Batch 400/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7407, Loss: 0.2223\n",
      "Epoch  12 Batch 600/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7556, Loss: 0.2670\n",
      "Epoch  12 Batch 800/1027 - Train Accuracy: 0.7758, Validation Accuracy: 0.7556, Loss: 0.2897\n",
      "Epoch  12 Batch 1000/1027 - Train Accuracy: 0.8422, Validation Accuracy: 0.7481, Loss: 0.2547\n",
      "Epoch  13 Batch 200/1027 - Train Accuracy: 0.7333, Validation Accuracy: 0.7556, Loss: 0.3918\n",
      "Epoch  13 Batch 400/1027 - Train Accuracy: 0.8455, Validation Accuracy: 0.7593, Loss: 0.2305\n",
      "Epoch  13 Batch 600/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7556, Loss: 0.2788\n",
      "Epoch  13 Batch 800/1027 - Train Accuracy: 0.7636, Validation Accuracy: 0.7630, Loss: 0.2786\n",
      "Epoch  13 Batch 1000/1027 - Train Accuracy: 0.8533, Validation Accuracy: 0.7556, Loss: 0.2442\n",
      "Epoch  14 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7593, Loss: 0.4114\n",
      "Epoch  14 Batch 400/1027 - Train Accuracy: 0.8667, Validation Accuracy: 0.7556, Loss: 0.1934\n",
      "Epoch  14 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7556, Loss: 0.2752\n",
      "Epoch  14 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7630, Loss: 0.2837\n",
      "Epoch  14 Batch 1000/1027 - Train Accuracy: 0.8756, Validation Accuracy: 0.7556, Loss: 0.2061\n",
      "Epoch  15 Batch 200/1027 - Train Accuracy: 0.7433, Validation Accuracy: 0.7630, Loss: 0.3380\n",
      "Epoch  15 Batch 400/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7556, Loss: 0.1992\n",
      "Epoch  15 Batch 600/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7741, Loss: 0.2462\n",
      "Epoch  15 Batch 800/1027 - Train Accuracy: 0.8212, Validation Accuracy: 0.7630, Loss: 0.2521\n",
      "Epoch  15 Batch 1000/1027 - Train Accuracy: 0.8467, Validation Accuracy: 0.7630, Loss: 0.1870\n",
      "Epoch  16 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7852, Loss: 0.3480\n",
      "Epoch  16 Batch 400/1027 - Train Accuracy: 0.8455, Validation Accuracy: 0.7519, Loss: 0.1867\n",
      "Epoch  16 Batch 600/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7593, Loss: 0.2447\n",
      "Epoch  16 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7593, Loss: 0.2394\n",
      "Epoch  16 Batch 1000/1027 - Train Accuracy: 0.8511, Validation Accuracy: 0.7630, Loss: 0.1908\n",
      "Epoch  17 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7593, Loss: 0.3080\n",
      "Epoch  17 Batch 400/1027 - Train Accuracy: 0.8667, Validation Accuracy: 0.7370, Loss: 0.2113\n",
      "Epoch  17 Batch 600/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7556, Loss: 0.2535\n",
      "Epoch  17 Batch 800/1027 - Train Accuracy: 0.7879, Validation Accuracy: 0.7481, Loss: 0.2300\n",
      "Epoch  17 Batch 1000/1027 - Train Accuracy: 0.8822, Validation Accuracy: 0.7630, Loss: 0.1878\n",
      "Epoch  18 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7630, Loss: 0.3017\n",
      "Epoch  18 Batch 400/1027 - Train Accuracy: 0.8667, Validation Accuracy: 0.7519, Loss: 0.2201\n",
      "Epoch  18 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18 Batch 800/1027 - Train Accuracy: 0.7939, Validation Accuracy: 0.7556, Loss: 0.2389\n",
      "Epoch  18 Batch 1000/1027 - Train Accuracy: 0.8778, Validation Accuracy: 0.7630, Loss: 0.1702\n",
      "Epoch  19 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7370, Loss: 0.3205\n",
      "Epoch  19 Batch 400/1027 - Train Accuracy: 0.8606, Validation Accuracy: 0.7519, Loss: 0.1822\n",
      "Epoch  19 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7556, Loss: 0.2327\n",
      "Epoch  19 Batch 800/1027 - Train Accuracy: 0.8273, Validation Accuracy: 0.7630, Loss: 0.2177\n",
      "Epoch  19 Batch 1000/1027 - Train Accuracy: 0.8689, Validation Accuracy: 0.7667, Loss: 0.1780\n",
      "Epoch  20 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7333, Loss: 0.3115\n",
      "Epoch  20 Batch 400/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7667, Loss: 0.1759\n",
      "Epoch  20 Batch 600/1027 - Train Accuracy: 0.8879, Validation Accuracy: 0.7222, Loss: 0.2222\n",
      "Epoch  20 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7630, Loss: 0.2395\n",
      "Epoch  20 Batch 1000/1027 - Train Accuracy: 0.8400, Validation Accuracy: 0.7630, Loss: 0.1720\n",
      "Epoch  21 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7630, Loss: 0.3092\n",
      "Epoch  21 Batch 400/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7519, Loss: 0.1874\n",
      "Epoch  21 Batch 600/1027 - Train Accuracy: 0.8606, Validation Accuracy: 0.7481, Loss: 0.2402\n",
      "Epoch  21 Batch 800/1027 - Train Accuracy: 0.7939, Validation Accuracy: 0.7630, Loss: 0.2180\n",
      "Epoch  21 Batch 1000/1027 - Train Accuracy: 0.8956, Validation Accuracy: 0.7630, Loss: 0.1610\n",
      "Epoch  22 Batch 200/1027 - Train Accuracy: 0.7333, Validation Accuracy: 0.7370, Loss: 0.3418\n",
      "Epoch  22 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7667, Loss: 0.2126\n",
      "Epoch  22 Batch 600/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7630, Loss: 0.2427\n",
      "Epoch  22 Batch 800/1027 - Train Accuracy: 0.7939, Validation Accuracy: 0.7667, Loss: 0.2013\n",
      "Epoch  22 Batch 1000/1027 - Train Accuracy: 0.8956, Validation Accuracy: 0.7667, Loss: 0.1548\n",
      "Epoch  23 Batch 200/1027 - Train Accuracy: 0.7100, Validation Accuracy: 0.7593, Loss: 0.3240\n",
      "Epoch  23 Batch 400/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7407, Loss: 0.1832\n",
      "Epoch  23 Batch 600/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7630, Loss: 0.2548\n",
      "Epoch  23 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7593, Loss: 0.1965\n",
      "Epoch  23 Batch 1000/1027 - Train Accuracy: 0.8844, Validation Accuracy: 0.7593, Loss: 0.1724\n",
      "Epoch  24 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7593, Loss: 0.3311\n",
      "Epoch  24 Batch 400/1027 - Train Accuracy: 0.8606, Validation Accuracy: 0.7630, Loss: 0.1580\n",
      "Epoch  24 Batch 600/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7667, Loss: 0.2251\n",
      "Epoch  24 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7630, Loss: 0.2028\n",
      "Epoch  24 Batch 1000/1027 - Train Accuracy: 0.8800, Validation Accuracy: 0.7667, Loss: 0.1758\n",
      "Epoch  25 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7593, Loss: 0.2826\n",
      "Epoch  25 Batch 400/1027 - Train Accuracy: 0.8606, Validation Accuracy: 0.7519, Loss: 0.1695\n",
      "Epoch  25 Batch 600/1027 - Train Accuracy: 0.8818, Validation Accuracy: 0.7630, Loss: 0.2469\n",
      "Epoch  25 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7630, Loss: 0.1732\n",
      "Epoch  25 Batch 1000/1027 - Train Accuracy: 0.8689, Validation Accuracy: 0.7630, Loss: 0.1877\n",
      "Epoch  26 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.3479\n",
      "Epoch  26 Batch 400/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7259, Loss: 0.2227\n",
      "Epoch  26 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7444, Loss: 0.2646\n",
      "Epoch  26 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7630, Loss: 0.1869\n",
      "Epoch  26 Batch 1000/1027 - Train Accuracy: 0.8889, Validation Accuracy: 0.7630, Loss: 0.1695\n",
      "Epoch  27 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.2694\n",
      "Epoch  27 Batch 400/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7481, Loss: 0.1490\n",
      "Epoch  27 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2423\n",
      "Epoch  27 Batch 800/1027 - Train Accuracy: 0.8273, Validation Accuracy: 0.7481, Loss: 0.1778\n",
      "Epoch  27 Batch 1000/1027 - Train Accuracy: 0.8844, Validation Accuracy: 0.7630, Loss: 0.1846\n",
      "Epoch  28 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.2790\n",
      "Epoch  28 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7556, Loss: 0.1824\n",
      "Epoch  28 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7630, Loss: 0.2114\n",
      "Epoch  28 Batch 800/1027 - Train Accuracy: 0.8000, Validation Accuracy: 0.7593, Loss: 0.1871\n",
      "Epoch  28 Batch 1000/1027 - Train Accuracy: 0.8778, Validation Accuracy: 0.7667, Loss: 0.1763\n",
      "Epoch  29 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.3164\n",
      "Epoch  29 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7556, Loss: 0.1591\n",
      "Epoch  29 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7630, Loss: 0.2392\n",
      "Epoch  29 Batch 800/1027 - Train Accuracy: 0.8182, Validation Accuracy: 0.7556, Loss: 0.1770\n",
      "Epoch  29 Batch 1000/1027 - Train Accuracy: 0.8644, Validation Accuracy: 0.7630, Loss: 0.1465\n",
      "Epoch  30 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7593, Loss: 0.2879\n",
      "Epoch  30 Batch 400/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7704, Loss: 0.1484\n",
      "Epoch  30 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2328\n",
      "Epoch  30 Batch 800/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7593, Loss: 0.1865\n",
      "Epoch  30 Batch 1000/1027 - Train Accuracy: 0.8778, Validation Accuracy: 0.7630, Loss: 0.1469\n",
      "Epoch  31 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.3160\n",
      "Epoch  31 Batch 400/1027 - Train Accuracy: 0.8364, Validation Accuracy: 0.7630, Loss: 0.1886\n",
      "Epoch  31 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7444, Loss: 0.2233\n",
      "Epoch  31 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7481, Loss: 0.1562\n",
      "Epoch  31 Batch 1000/1027 - Train Accuracy: 0.8844, Validation Accuracy: 0.7667, Loss: 0.1506\n",
      "Epoch  32 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7852, Loss: 0.3151\n",
      "Epoch  32 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7444, Loss: 0.1534\n",
      "Epoch  32 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2294\n",
      "Epoch  32 Batch 800/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7593, Loss: 0.1480\n",
      "Epoch  32 Batch 1000/1027 - Train Accuracy: 0.8733, Validation Accuracy: 0.7889, Loss: 0.1579\n",
      "Epoch  33 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.2906\n",
      "Epoch  33 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7407, Loss: 0.1963\n",
      "Epoch  33 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2055\n",
      "Epoch  33 Batch 800/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7444, Loss: 0.2097\n",
      "Epoch  33 Batch 1000/1027 - Train Accuracy: 0.8800, Validation Accuracy: 0.7630, Loss: 0.1404\n",
      "Epoch  34 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7630, Loss: 0.2877\n",
      "Epoch  34 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7519, Loss: 0.1699\n",
      "Epoch  34 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7667, Loss: 0.2294\n",
      "Epoch  34 Batch 800/1027 - Train Accuracy: 0.7939, Validation Accuracy: 0.7481, Loss: 0.1697\n",
      "Epoch  34 Batch 1000/1027 - Train Accuracy: 0.8911, Validation Accuracy: 0.7630, Loss: 0.1262\n",
      "Epoch  35 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7593, Loss: 0.2791\n",
      "Epoch  35 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7593, Loss: 0.1543\n",
      "Epoch  35 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7630, Loss: 0.2351\n",
      "Epoch  35 Batch 800/1027 - Train Accuracy: 0.8152, Validation Accuracy: 0.7593, Loss: 0.1686\n",
      "Epoch  35 Batch 1000/1027 - Train Accuracy: 0.8733, Validation Accuracy: 0.7630, Loss: 0.1472\n",
      "Epoch  36 Batch 200/1027 - Train Accuracy: 0.7633, Validation Accuracy: 0.7815, Loss: 0.3040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  36 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7519, Loss: 0.1827\n",
      "Epoch  36 Batch 600/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7630, Loss: 0.2497\n",
      "Epoch  36 Batch 800/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7519, Loss: 0.1779\n",
      "Epoch  36 Batch 1000/1027 - Train Accuracy: 0.8733, Validation Accuracy: 0.7667, Loss: 0.1574\n",
      "Epoch  37 Batch 200/1027 - Train Accuracy: 0.7433, Validation Accuracy: 0.7519, Loss: 0.3296\n",
      "Epoch  37 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7519, Loss: 0.2081\n",
      "Epoch  37 Batch 600/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7741, Loss: 0.2212\n",
      "Epoch  37 Batch 800/1027 - Train Accuracy: 0.8000, Validation Accuracy: 0.7593, Loss: 0.1780\n",
      "Epoch  37 Batch 1000/1027 - Train Accuracy: 0.8756, Validation Accuracy: 0.7593, Loss: 0.1384\n",
      "Epoch  38 Batch 200/1027 - Train Accuracy: 0.7767, Validation Accuracy: 0.7704, Loss: 0.3057\n",
      "Epoch  38 Batch 400/1027 - Train Accuracy: 0.8606, Validation Accuracy: 0.7519, Loss: 0.1548\n",
      "Epoch  38 Batch 600/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7593, Loss: 0.1980\n",
      "Epoch  38 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7593, Loss: 0.1785\n",
      "Epoch  38 Batch 1000/1027 - Train Accuracy: 0.8800, Validation Accuracy: 0.7741, Loss: 0.1330\n",
      "Epoch  39 Batch 200/1027 - Train Accuracy: 0.7733, Validation Accuracy: 0.7593, Loss: 0.2848\n",
      "Epoch  39 Batch 400/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7407, Loss: 0.1673\n",
      "Epoch  39 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7630, Loss: 0.2049\n",
      "Epoch  39 Batch 800/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7593, Loss: 0.1669\n",
      "Epoch  39 Batch 1000/1027 - Train Accuracy: 0.8644, Validation Accuracy: 0.7630, Loss: 0.1425\n",
      "Epoch  40 Batch 200/1027 - Train Accuracy: 0.7300, Validation Accuracy: 0.7593, Loss: 0.2945\n",
      "Epoch  40 Batch 400/1027 - Train Accuracy: 0.8758, Validation Accuracy: 0.7630, Loss: 0.1764\n",
      "Epoch  40 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7556, Loss: 0.2084\n",
      "Epoch  40 Batch 800/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7593, Loss: 0.1840\n",
      "Epoch  40 Batch 1000/1027 - Train Accuracy: 0.8489, Validation Accuracy: 0.7741, Loss: 0.1516\n",
      "Epoch  41 Batch 200/1027 - Train Accuracy: 0.7733, Validation Accuracy: 0.7815, Loss: 0.2813\n",
      "Epoch  41 Batch 400/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7185, Loss: 0.1501\n",
      "Epoch  41 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2143\n",
      "Epoch  41 Batch 800/1027 - Train Accuracy: 0.8273, Validation Accuracy: 0.7593, Loss: 0.1881\n",
      "Epoch  41 Batch 1000/1027 - Train Accuracy: 0.8844, Validation Accuracy: 0.7630, Loss: 0.1674\n",
      "Epoch  42 Batch 200/1027 - Train Accuracy: 0.7667, Validation Accuracy: 0.7259, Loss: 0.2554\n",
      "Epoch  42 Batch 400/1027 - Train Accuracy: 0.8455, Validation Accuracy: 0.7481, Loss: 0.1557\n",
      "Epoch  42 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7519, Loss: 0.2104\n",
      "Epoch  42 Batch 800/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7593, Loss: 0.1563\n",
      "Epoch  42 Batch 1000/1027 - Train Accuracy: 0.8933, Validation Accuracy: 0.7593, Loss: 0.1435\n",
      "Epoch  43 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7593, Loss: 0.2669\n",
      "Epoch  43 Batch 400/1027 - Train Accuracy: 0.8455, Validation Accuracy: 0.7556, Loss: 0.1514\n",
      "Epoch  43 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2018\n",
      "Epoch  43 Batch 800/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7593, Loss: 0.1655\n",
      "Epoch  43 Batch 1000/1027 - Train Accuracy: 0.8689, Validation Accuracy: 0.7593, Loss: 0.1512\n",
      "Epoch  44 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7741, Loss: 0.2942\n",
      "Epoch  44 Batch 400/1027 - Train Accuracy: 0.8424, Validation Accuracy: 0.7222, Loss: 0.1605\n",
      "Epoch  44 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7741, Loss: 0.1784\n",
      "Epoch  44 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7519, Loss: 0.1526\n",
      "Epoch  44 Batch 1000/1027 - Train Accuracy: 0.9067, Validation Accuracy: 0.7778, Loss: 0.1139\n",
      "Epoch  45 Batch 200/1027 - Train Accuracy: 0.7633, Validation Accuracy: 0.7556, Loss: 0.2611\n",
      "Epoch  45 Batch 400/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7630, Loss: 0.1827\n",
      "Epoch  45 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.1770\n",
      "Epoch  45 Batch 800/1027 - Train Accuracy: 0.8242, Validation Accuracy: 0.7593, Loss: 0.1606\n",
      "Epoch  45 Batch 1000/1027 - Train Accuracy: 0.8933, Validation Accuracy: 0.7593, Loss: 0.1352\n",
      "Epoch  46 Batch 200/1027 - Train Accuracy: 0.7867, Validation Accuracy: 0.7593, Loss: 0.2905\n",
      "Epoch  46 Batch 400/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7593, Loss: 0.1627\n",
      "Epoch  46 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7593, Loss: 0.2193\n",
      "Epoch  46 Batch 800/1027 - Train Accuracy: 0.7970, Validation Accuracy: 0.7593, Loss: 0.1616\n",
      "Epoch  46 Batch 1000/1027 - Train Accuracy: 0.8911, Validation Accuracy: 0.7741, Loss: 0.1212\n",
      "Epoch  47 Batch 200/1027 - Train Accuracy: 0.7733, Validation Accuracy: 0.7593, Loss: 0.2818\n",
      "Epoch  47 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7148, Loss: 0.1433\n",
      "Epoch  47 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7519, Loss: 0.1996\n",
      "Epoch  47 Batch 800/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7593, Loss: 0.1402\n",
      "Epoch  47 Batch 1000/1027 - Train Accuracy: 0.8756, Validation Accuracy: 0.7593, Loss: 0.1379\n",
      "Epoch  48 Batch 200/1027 - Train Accuracy: 0.7633, Validation Accuracy: 0.7741, Loss: 0.2387\n",
      "Epoch  48 Batch 400/1027 - Train Accuracy: 0.8758, Validation Accuracy: 0.7222, Loss: 0.1557\n",
      "Epoch  48 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2019\n",
      "Epoch  48 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7630, Loss: 0.1624\n",
      "Epoch  48 Batch 1000/1027 - Train Accuracy: 0.9044, Validation Accuracy: 0.7741, Loss: 0.1065\n",
      "Epoch  49 Batch 200/1027 - Train Accuracy: 0.7733, Validation Accuracy: 0.7556, Loss: 0.3308\n",
      "Epoch  49 Batch 400/1027 - Train Accuracy: 0.8758, Validation Accuracy: 0.7593, Loss: 0.1393\n",
      "Epoch  49 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7593, Loss: 0.1808\n",
      "Epoch  49 Batch 800/1027 - Train Accuracy: 0.8030, Validation Accuracy: 0.7630, Loss: 0.1664\n",
      "Epoch  49 Batch 1000/1027 - Train Accuracy: 0.8867, Validation Accuracy: 0.7667, Loss: 0.1811\n",
      "Epoch  50 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7593, Loss: 0.2880\n",
      "Epoch  50 Batch 400/1027 - Train Accuracy: 0.8515, Validation Accuracy: 0.7741, Loss: 0.1734\n",
      "Epoch  50 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2153\n",
      "Epoch  50 Batch 800/1027 - Train Accuracy: 0.8303, Validation Accuracy: 0.7593, Loss: 0.1615\n",
      "Epoch  50 Batch 1000/1027 - Train Accuracy: 0.8956, Validation Accuracy: 0.7741, Loss: 0.1347\n",
      "Epoch  51 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7741, Loss: 0.3127\n",
      "Epoch  51 Batch 400/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7667, Loss: 0.1622\n",
      "Epoch  51 Batch 600/1027 - Train Accuracy: 0.8818, Validation Accuracy: 0.7630, Loss: 0.1817\n",
      "Epoch  51 Batch 800/1027 - Train Accuracy: 0.8091, Validation Accuracy: 0.7593, Loss: 0.1482\n",
      "Epoch  51 Batch 1000/1027 - Train Accuracy: 0.8911, Validation Accuracy: 0.7741, Loss: 0.1296\n",
      "Epoch  52 Batch 200/1027 - Train Accuracy: 0.7600, Validation Accuracy: 0.7593, Loss: 0.2366\n",
      "Epoch  52 Batch 400/1027 - Train Accuracy: 0.8636, Validation Accuracy: 0.7556, Loss: 0.1596\n",
      "Epoch  52 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7444, Loss: 0.1818\n",
      "Epoch  52 Batch 800/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7593, Loss: 0.1624\n",
      "Epoch  52 Batch 1000/1027 - Train Accuracy: 0.9111, Validation Accuracy: 0.7593, Loss: 0.0954\n",
      "Epoch  53 Batch 200/1027 - Train Accuracy: 0.7467, Validation Accuracy: 0.7593, Loss: 0.2814\n",
      "Epoch  53 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7556, Loss: 0.1794\n",
      "Epoch  53 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.1741\n",
      "Epoch  53 Batch 800/1027 - Train Accuracy: 0.8545, Validation Accuracy: 0.7630, Loss: 0.1573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  53 Batch 1000/1027 - Train Accuracy: 0.8889, Validation Accuracy: 0.7630, Loss: 0.1012\n",
      "Epoch  54 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7741, Loss: 0.2693\n",
      "Epoch  54 Batch 400/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7593, Loss: 0.1384\n",
      "Epoch  54 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7593, Loss: 0.1798\n",
      "Epoch  54 Batch 800/1027 - Train Accuracy: 0.8576, Validation Accuracy: 0.7593, Loss: 0.1646\n",
      "Epoch  54 Batch 1000/1027 - Train Accuracy: 0.9022, Validation Accuracy: 0.7741, Loss: 0.1078\n",
      "Epoch  55 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7741, Loss: 0.2604\n",
      "Epoch  55 Batch 400/1027 - Train Accuracy: 0.8212, Validation Accuracy: 0.7593, Loss: 0.1670\n",
      "Epoch  55 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2029\n",
      "Epoch  55 Batch 800/1027 - Train Accuracy: 0.8394, Validation Accuracy: 0.7556, Loss: 0.1358\n",
      "Epoch  55 Batch 1000/1027 - Train Accuracy: 0.8844, Validation Accuracy: 0.7593, Loss: 0.0974\n",
      "Epoch  56 Batch 200/1027 - Train Accuracy: 0.7567, Validation Accuracy: 0.7593, Loss: 0.2751\n",
      "Epoch  56 Batch 400/1027 - Train Accuracy: 0.8485, Validation Accuracy: 0.7593, Loss: 0.1974\n",
      "Epoch  56 Batch 600/1027 - Train Accuracy: 0.8788, Validation Accuracy: 0.7593, Loss: 0.1871\n",
      "Epoch  56 Batch 800/1027 - Train Accuracy: 0.8091, Validation Accuracy: 0.7630, Loss: 0.2235\n",
      "Epoch  56 Batch 1000/1027 - Train Accuracy: 0.9133, Validation Accuracy: 0.7778, Loss: 0.1083\n",
      "Epoch  57 Batch 200/1027 - Train Accuracy: 0.7333, Validation Accuracy: 0.7741, Loss: 0.2550\n",
      "Epoch  57 Batch 400/1027 - Train Accuracy: 0.8667, Validation Accuracy: 0.7630, Loss: 0.1438\n",
      "Epoch  57 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.2131\n",
      "Epoch  57 Batch 800/1027 - Train Accuracy: 0.8242, Validation Accuracy: 0.7593, Loss: 0.1795\n",
      "Epoch  57 Batch 1000/1027 - Train Accuracy: 0.8822, Validation Accuracy: 0.7778, Loss: 0.1075\n",
      "Epoch  58 Batch 200/1027 - Train Accuracy: 0.7533, Validation Accuracy: 0.7593, Loss: 0.2207\n",
      "Epoch  58 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7630, Loss: 0.1282\n",
      "Epoch  58 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7444, Loss: 0.1898\n",
      "Epoch  58 Batch 800/1027 - Train Accuracy: 0.8364, Validation Accuracy: 0.7444, Loss: 0.1560\n",
      "Epoch  58 Batch 1000/1027 - Train Accuracy: 0.8867, Validation Accuracy: 0.7778, Loss: 0.1204\n",
      "Epoch  59 Batch 200/1027 - Train Accuracy: 0.7400, Validation Accuracy: 0.7444, Loss: 0.2525\n",
      "Epoch  59 Batch 400/1027 - Train Accuracy: 0.8727, Validation Accuracy: 0.7630, Loss: 0.1624\n",
      "Epoch  59 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.1850\n",
      "Epoch  59 Batch 800/1027 - Train Accuracy: 0.8091, Validation Accuracy: 0.7630, Loss: 0.1674\n",
      "Epoch  59 Batch 1000/1027 - Train Accuracy: 0.9089, Validation Accuracy: 0.7630, Loss: 0.1024\n",
      "Epoch  60 Batch 200/1027 - Train Accuracy: 0.7500, Validation Accuracy: 0.7741, Loss: 0.2366\n",
      "Epoch  60 Batch 400/1027 - Train Accuracy: 0.8697, Validation Accuracy: 0.7741, Loss: 0.1266\n",
      "Epoch  60 Batch 600/1027 - Train Accuracy: 0.8939, Validation Accuracy: 0.7593, Loss: 0.1637\n",
      "Epoch  60 Batch 800/1027 - Train Accuracy: 0.8333, Validation Accuracy: 0.7704, Loss: 0.1614\n",
      "Epoch  60 Batch 1000/1027 - Train Accuracy: 0.8867, Validation Accuracy: 0.7556, Loss: 0.1250\n",
      "Model Trained and saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "                print('Epoch {:>3} Batch {:>3}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i+1, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and saved')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters   \n",
    "def save_params(params):\n",
    "    with open('paramsTestNew.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "save_params(save_path)\n",
    "def load_params():\n",
    "    with open('paramsTestNew.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_int_text, target_int_text, source_vocab_to_int, target_vocab_to_int,source_int_to_vocab,target_int_to_vocab = preprocess(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = load_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpointsNew2/dev'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Enter word to be transliterated:\n",
      "INFO:tensorflow:Restoring parameters from checkpointsNew2/dev\n",
      "Input\n",
      "  Word Ids:      [31, 74, 26, 52, 11, 74]\n",
      "  Hindi Word: ['ज', 'ा', 'म', 'ि', 'य', 'ा']\n",
      "\n",
      "Prediction\n",
      "  Word Id:      [15, 12, 12, 21, 18, 27]\n",
      "  English Word:      jaamiy\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "#converting the words to vectors of integers\n",
    "def word_to_seq(word, vocab_to_int):\n",
    "    results = []\n",
    "    for word in list(word):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "#taking user input for prediction\n",
    "print(\"\\n Enter word to be transliterated:\")\n",
    "transliterate_word = 'जामिया'\n",
    "transliterate_word = word_to_seq(transliterate_word, source_vocab_to_int)\n",
    "\n",
    "#initialising the graph\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "#initialising the session\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "        \n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    \n",
    "    #tf.train.Saver.restore(sess,load_path)\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "#providing placeholder names from the loaded graph\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "#transliterating the given word\n",
    "    transliterate_logits = sess.run(logits, {input_data: [transliterate_word]*batch_size,\n",
    "                                         target_sequence_length: [len(transliterate_word)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in transliterate_word]))\n",
    "print('  Hindi Word: {}'.format([source_int_to_vocab[i] for i in transliterate_word]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Id:      {}'.format([i for i in transliterate_logits]))\n",
    "\n",
    "#showing the output\n",
    "output = \"\"\n",
    "for i in transliterate_logits:\n",
    "        if target_int_to_vocab[i]!= '<EOS>':\n",
    "                output = output + target_int_to_vocab[i]\n",
    "print('  English Word:      {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<PAD>': 0,\n",
       "  '<EOS>': 1,\n",
       "  '<UNK>': 2,\n",
       "  '<GO>': 3,\n",
       "  'ॉ': 4,\n",
       "  'ख': 5,\n",
       "  'ू': 6,\n",
       "  'ॆ': 7,\n",
       "  'ः': 8,\n",
       "  'स': 9,\n",
       "  'ह': 10,\n",
       "  'य': 11,\n",
       "  'छ': 12,\n",
       "  'ल': 13,\n",
       "  'श': 14,\n",
       "  'ऻ': 15,\n",
       "  'ण': 16,\n",
       "  'ॊ': 17,\n",
       "  'ु': 18,\n",
       "  'ई': 19,\n",
       "  'त': 20,\n",
       "  'ॏ': 21,\n",
       "  'ट': 22,\n",
       "  'ृ': 23,\n",
       "  'ो': 24,\n",
       "  'म': 25,\n",
       "  'ड': 26,\n",
       "  'ौ': 27,\n",
       "  'उ': 28,\n",
       "  'ए': 29,\n",
       "  'ज': 30,\n",
       "  'ै': 31,\n",
       "  'ञ': 32,\n",
       "  'च': 33,\n",
       "  'ग़': 34,\n",
       "  'प': 35,\n",
       "  'ऑ': 36,\n",
       "  'ठ': 37,\n",
       "  'ळ': 38,\n",
       "  'औ': 39,\n",
       "  'ध': 40,\n",
       "  'व': 41,\n",
       "  'थ': 42,\n",
       "  'ऐ': 43,\n",
       "  'न': 44,\n",
       "  '़': 45,\n",
       "  'क़': 46,\n",
       "  'द': 47,\n",
       "  'ऺ': 48,\n",
       "  'अ': 49,\n",
       "  'ि': 50,\n",
       "  'ड़': 51,\n",
       "  'क': 52,\n",
       "  'इ': 53,\n",
       "  'र': 54,\n",
       "  'भ': 55,\n",
       "  'आ': 56,\n",
       "  'े': 57,\n",
       "  'घ': 58,\n",
       "  'ख़': 59,\n",
       "  'ॎ': 60,\n",
       "  'ढ': 61,\n",
       "  'ज़': 62,\n",
       "  'ी': 63,\n",
       "  'फ़': 64,\n",
       "  'फ': 65,\n",
       "  '्': 66,\n",
       "  'झ': 67,\n",
       "  'ग': 68,\n",
       "  'ऊ': 69,\n",
       "  'ष': 70,\n",
       "  'ढ़': 71,\n",
       "  'ा': 72,\n",
       "  'ओ': 73,\n",
       "  'ब': 74},\n",
       " {0: '<PAD>',\n",
       "  1: '<EOS>',\n",
       "  2: '<UNK>',\n",
       "  3: '<GO>',\n",
       "  4: 'ॉ',\n",
       "  5: 'ख',\n",
       "  6: 'ू',\n",
       "  7: 'ॆ',\n",
       "  8: 'ः',\n",
       "  9: 'स',\n",
       "  10: 'ह',\n",
       "  11: 'य',\n",
       "  12: 'छ',\n",
       "  13: 'ल',\n",
       "  14: 'श',\n",
       "  15: 'ऻ',\n",
       "  16: 'ण',\n",
       "  17: 'ॊ',\n",
       "  18: 'ु',\n",
       "  19: 'ई',\n",
       "  20: 'त',\n",
       "  21: 'ॏ',\n",
       "  22: 'ट',\n",
       "  23: 'ृ',\n",
       "  24: 'ो',\n",
       "  25: 'म',\n",
       "  26: 'ड',\n",
       "  27: 'ौ',\n",
       "  28: 'उ',\n",
       "  29: 'ए',\n",
       "  30: 'ज',\n",
       "  31: 'ै',\n",
       "  32: 'ञ',\n",
       "  33: 'च',\n",
       "  34: 'ग़',\n",
       "  35: 'प',\n",
       "  36: 'ऑ',\n",
       "  37: 'ठ',\n",
       "  38: 'ळ',\n",
       "  39: 'औ',\n",
       "  40: 'ध',\n",
       "  41: 'व',\n",
       "  42: 'थ',\n",
       "  43: 'ऐ',\n",
       "  44: 'न',\n",
       "  45: '़',\n",
       "  46: 'क़',\n",
       "  47: 'द',\n",
       "  48: 'ऺ',\n",
       "  49: 'अ',\n",
       "  50: 'ि',\n",
       "  51: 'ड़',\n",
       "  52: 'क',\n",
       "  53: 'इ',\n",
       "  54: 'र',\n",
       "  55: 'भ',\n",
       "  56: 'आ',\n",
       "  57: 'े',\n",
       "  58: 'घ',\n",
       "  59: 'ख़',\n",
       "  60: 'ॎ',\n",
       "  61: 'ढ',\n",
       "  62: 'ज़',\n",
       "  63: 'ी',\n",
       "  64: 'फ़',\n",
       "  65: 'फ',\n",
       "  66: '्',\n",
       "  67: 'झ',\n",
       "  68: 'ग',\n",
       "  69: 'ऊ',\n",
       "  70: 'ष',\n",
       "  71: 'ढ़',\n",
       "  72: 'ा',\n",
       "  73: 'ओ',\n",
       "  74: 'ब'})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lookup_table('ॉॆॊॏऺऻॎःािीुूेैोौअआइईउऊएऐओऔकखगघचछजझटठडढणतथदधनपफबभमयरलवशषसहज्ञक्षश्रज़रफ़ड़ढ़ख़क़ग़ळृृ़़ऑ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
