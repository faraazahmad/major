{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "\n",
    "# pandas used for reading data\n",
    "import pandas as pd\n",
    "\n",
    "#pickle for dumping(saving) data in a file\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#Reading dataset\n",
    "dataset = pd.read_csv(\"transliteration.txt\",delimiter = \"\\t\",header=None,encoding='utf-8',na_filter = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting English words in X and Hindi words in y\n",
    "X = dataset.iloc[:,0]\n",
    "y = dataset.iloc[:,-1]\n",
    "\n",
    "#importing the preprocessed data \n",
    "#The preprocessing is done in a file named Data_preprocessing\n",
    "import Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_int_text is the English words' processed vector. (Word is Converted to integer vector)\n",
    "#target_int_text is the Hindi words' processed vector.\n",
    "#source_vocab_to_int and  source_int_to_vocab are the English dictionaries\n",
    "#target_vocab_to_int and  target_int_to_vocab are the Hindi dictionaries\n",
    "\n",
    "source_int_text, target_int_text, source_vocab_to_int, target_vocab_to_int,source_int_to_vocab,target_int_to_vocab = Data_preprocessing.preprocess(X,y)\n",
    "\n",
    "#encoder and decoder layers are defined in Layers \n",
    "#placeholders are defined in Model_Inputs\n",
    "import Layers\n",
    "import Model_Inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_word_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "\n",
    "#    Build the Sequence-to-Sequence model\n",
    "#    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    enc_outputs, enc_states = Layers.encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = Model_Inputs.process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = Layers.decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_word_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the parameters\n",
    "display_step = 200\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 30\n",
    "\n",
    "rnn_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "encoding_embedding_size = 50\n",
    "decoding_embedding_size = 50\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n",
    "\n",
    "\n",
    "# path for saving the model\n",
    "save_path = 'checkpoints/dev'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/asheeque/anaconda3/envs/major/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_v2_behavior() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "#   English Inputs\n",
    "    inputs = tf.compat.v1.placeholder(tf.int32, [None, None], name='input')\n",
    "    \n",
    "#   Hindi Inputs\n",
    "    targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='targets') \n",
    "\n",
    "#   Length of Hindi words\n",
    "    target_sequence_length = tf.compat.v1.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    \n",
    "    #computes the maximum of all elements across the dimension\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len\n",
    "\n",
    "def hyperparam_inputs():\n",
    "    \n",
    "    #learning rate\n",
    "    lr_rate = tf.compat.v1.placeholder(tf.float32, name='lr_rate')\n",
    "    \n",
    "    #probability of dropout\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob\n",
    "\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "\n",
    "#    Preprocess target data for encoding\n",
    "#    :return: Preprocessed target data\n",
    "        \n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    #Take the batch of Hindi target\n",
    "    #tf.strided_slice(input_,begin,end,strides=None)\n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    \n",
    "    #Concat every word of the Hindi target with <GO> id \n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#initialising the graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length =enc_dec_model_inputs()\n",
    "#   taking learning rate and probability of drop out layer    \n",
    "    lr, keep_prob = hyperparam_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1c825cfcafea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                    \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                    \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                    \u001b[0mtarget_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d33b1fa2f840>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(input_data, target_data, keep_prob, batch_size, target_sequence_length, max_target_word_length, source_vocab_size, target_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#    Build the Sequence-to-Sequence model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     enc_outputs, enc_states = Layers.encoding_layer(input_data, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                              \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                              \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/major/major/lstm/Layers.py\u001b[0m in \u001b[0;36mencoding_layer\u001b[0;34m(rnn_inputs, rnn_size, num_layers, keep_prob, source_vocab_size, encoding_embedding_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#    :return: tuple (RNN output, RNN state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n\u001b[0m\u001b[1;32m     12\u001b[0m                                              \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                              embed_dim=encoding_embedding_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "\n",
    "masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
