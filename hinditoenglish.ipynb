{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wikipedia API instance\n",
    "wiki = wikipediaapi.Wikipedia('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get English title of given Hindi page\n",
    "def get_eng_title(page):\n",
    "    langlinks = page.langlinks\n",
    "    \n",
    "    if 'en' in langlinks.keys():\n",
    "        return langlinks['en'].title\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_links(page,body):\n",
    "    links = []\n",
    "    for link in page.links:\n",
    "        if body.find(link) != -1:\n",
    "            links.append(link)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(hindi_list):\n",
    "    # list to return hindi-english mappings in\n",
    "    word_mappings = []\n",
    "    \n",
    "    for hindi_title in hindi_list:\n",
    "        page = wiki.page(hindi_title)\n",
    "        \n",
    "        # if page doesn't exist for this hindi title, skip it\n",
    "        try:\n",
    "            page.exists()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        # get english title of hindi topic\n",
    "        eng_title = get_eng_title(page)\n",
    "        # if there is no english title, skip it\n",
    "        if eng_title is None:\n",
    "            continue\n",
    "        # append final map list with english and hindi titles\n",
    "        word_mappings.append([hindi_title, eng_title])\n",
    "    return word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list(['ब्रिटेन','ब्रिटेन'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code mixed corpus by replacing hindi word in body with english one\n",
    "def find_and_replace(body, word_list):\n",
    "    for word in word_list:\n",
    "        body = body.replace(word[0], word[1])\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file with english names\n",
    "names_file = open('cand.txt', \"r\", encoding = \"ISO-8859-1\")\n",
    "names = []\n",
    "\n",
    "# clean data and append in names list\n",
    "for name in names_file:\n",
    "    name = name.replace('\\n','').replace(\" \",\"_\")\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of english names\n",
    "names_df = pd.DataFrame()\n",
    "names_df['english'] = names\n",
    "\n",
    "names_df = names_df[0:20]\n",
    "names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hindi_name(x):\n",
    "    eng_wiki = wikipediaapi.Wikipedia('en')\n",
    "    page = eng_wiki.page(x)\n",
    "    \n",
    "    if 'hi' in page.langlinks:  \n",
    "        hindi_name = page.langlinks['hi'].title\n",
    "        print(hindi_name)\n",
    "        return hindi_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df['hindi'] = names_df.apply(get_hindi_name, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names_df = names_df[names_df.hindi != None]\n",
    "names_df = names_df.replace(to_replace='None', value=np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.index = range(len(names_df))\n",
    "names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.to_csv('parallel_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpus_list=[]\n",
    "\n",
    "# given a hindi title, get its mixed corpus\n",
    "def get_mixed_corpus(hindi_title):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('hi')\n",
    "    page_py = wiki_wiki.page(hindi_title)\n",
    "    body = page_py.text.replace('\\n','').replace('==', '').replace('\\u200d', '').replace('।', '')\n",
    "    print(hindi_title)\n",
    "    link_list=print_links(page_py,body)\n",
    "    \n",
    "    hindi_english_list=word_list(link_list)\n",
    "    word_corpus_list.append(hindi_english_list)\n",
    "    replaced_text=find_and_replace(body,hindi_english_list)\n",
    "\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_parallel = pd.read_csv('names_parallel.csv')\n",
    "# names_parallel.drop(names_parallel.columns[names_parallel.columns.str.contains('Unnamed', case=False)], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nth_batch(n):\n",
    "    return names_parallel['hindi'][709*n:(n+1)*709]\n",
    "\n",
    "def work_on_batch(n):\n",
    "    # n = nth batch from 2836 rows\n",
    "    hindi_names_list = get_nth_batch(n)\n",
    "    local_mixed_corpus_list = []\n",
    "    \n",
    "    for name in hindi_names_list:\n",
    "        local_mixed_corpus_list.append(get_mixed_corpus(name))\n",
    "    \n",
    "    # \n",
    "    return local_mixed_corpus_list\n",
    "\n",
    "def task_finished(task):\n",
    "    print(task.get_name(), ' finished')\n",
    "    print(datetime.now().strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नगेन्द्र सिंह\n",
      "नरेश चन्द्रा\n",
      "घनश्यामदास बिड़ला\n",
      "राम नारायण\n",
      "दौलत सिंह कोठारी\n",
      "पद्म भूषण\n",
      "मनमोहन शर्मा\n",
      "मोहन सिंह मेहता\n",
      "के॰ एल॰ श्रीमाली\n",
      "माणिक्यलाल वर्मा\n",
      "कोमल कोठारी\n",
      "दुर्गा लाल\n",
      "0:01:48.865060\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "cpu_cores = len(os.sched_getaffinity(0))\n",
    "n = 4 # number of batches\n",
    "\n",
    "# create no of processes equal to no of cpu cores\n",
    "with Pool(cpu_cores) as p:\n",
    "    p.map(work_on_batch, range(n))\n",
    "end = datetime.now()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नरेश चन्द्रा\n",
      "राम नारायण\n",
      "मनमोहन शर्मा\n",
      "के॰ एल॰ श्रीमाली\n",
      "नगेन्द्र सिंह\n",
      "दौलत सिंह कोठारी\n",
      "मोहन सिंह मेहता\n",
      "माणिक्यलाल वर्मा\n",
      "घनश्यामदास बिड़ला\n",
      "पद्म भूषण\n",
      "कोमल कोठारी\n",
      "दुर्गा लाल\n",
      "0:03:04.469945\n"
     ]
    }
   ],
   "source": [
    "mixed_corpus_list = []\n",
    "n = len(names_parallel)\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(12):\n",
    "    mixed_corpus_list.append(get_mixed_corpus(names_parallel['hindi'][i]))\n",
    "end = datetime.now()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_parallel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
